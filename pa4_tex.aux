\relax 
\citation{tsne}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Supplemental Material}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Model}{1}}
\newlabel{model}{{2}{1}}
\newlabel{bigcost}{{1}{1}}
\newlabel{singleNN}{{2}{1}}
\newlabel{doubleNN}{{3}{1}}
\citation{collobert}
\citation{collobert}
\@writefile{toc}{\contentsline {section}{\numberline {3}Netwok Analysis}{2}}
\newlabel{netanalysis}{{3}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces F1 scores of model with the presence and absence of capitalization matrix ( C = 0.0 W = 5 H = 100 A = 0.01)}}{2}}
\newlabel{capfig}{{1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces F1 scores of model with document and sentence based start and end tokens ( C = 0.0 W = 5 H = 100 A = 0.01)}}{2}}
\newlabel{sentencefig}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces F1 scores of the training set varying the learning rate for stochastic gradient ( C = 0.0001 W = 7 H = 100)}}{3}}
\newlabel{learntrain}{{3}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces F1 scores of the test set varying the learning rate for stochastic gradient ( C = 0.0001 W = 7 H = 100)}}{3}}
\newlabel{learndev}{{4}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces F1 scores of train and dev sets with a 1 hidden layer model varying the window size. ( C = 0.0 H = 100 A = 0.01)}}{4}}
\newlabel{winsize}{{5}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces F1 scores of train and dev sets with a 1 hidden layer model varying the hidden layer size. ( C = 0.0 W = 5 A = 0.01)}}{4}}
\newlabel{hiddensize}{{6}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces F1 scores of train and dev sets with a 1 hidden layer model varying the regularization constant. ( W = 5 H = 100 A = 0.01)}}{4}}
\newlabel{varyregularization}{{7}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces F1 scores of train and dev sets with both the single and double hidden layer NN architectures. (C=0.001 W = 7 H = 100 A = 0.01)}}{4}}
\newlabel{numhl}{{8}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Further Optimizations}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Error Analysis}{5}}
\bibcite{tsne}{1}
\bibcite{collobert}{2}
\@writefile{toc}{\contentsline {section}{\numberline {A}Backpropagation}{7}}
\newlabel{backprop}{{A}{7}}
\newlabel{tanhderiv}{{8}{7}}
\newlabel{sigderiv}{{9}{7}}
\newlabel{cost}{{10}{7}}
\newlabel{ureg}{{11}{7}}
\newlabel{wreg}{{12}{7}}
\newlabel{simplecost}{{14}{8}}
\newlabel{partialU}{{15}{8}}
\newlabel{dzdu}{{16}{8}}
\newlabel{djdu}{{17}{8}}
\newlabel{djdb2}{{18}{8}}
\newlabel{partialW}{{19}{8}}
\newlabel{dJdW}{{20}{8}}
\newlabel{dz2da1}{{21}{8}}
\newlabel{da1dz1}{{22}{9}}
\newlabel{dz1dw}{{23}{9}}
\newlabel{djdw}{{25}{9}}
\newlabel{dz1db1}{{27}{9}}
\newlabel{djdb1}{{28}{9}}
\newlabel{djdxi}{{31}{9}}
\newlabel{djdx}{{32}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Softmax and Logistic Regression}{10}}
\newlabel{softmaxproof}{{B}{10}}
\newlabel{softmax}{{35}{10}}
\newlabel{logderivation}{{37}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Word Vector Visualization Using t-SNE}{11}}
\newlabel{tsne}{{C}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Collaboration}{11}}
