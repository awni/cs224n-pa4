 \documentclass{article} 
\setlength{\topmargin}{-.5in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{.125in}
\setlength{\textwidth}{6.25in}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{appendix}
  \begin{document}
\pagestyle{plain}

\title{Learning Named Entity Recognition with Deep Neural Networks}
\author{Awni Hannun\\
{\small awni@stanford.edu}\\
\and
Ibrahim Cotran\\
{\small icotran@stanford.edu}\\
}
\maketitle
\section{Introduction}
Named entity recognition (NER) has strong applications in natural language processing including Machine Translation and semantic representation of language. Here we explore the application of single and multi-layer Neural Networks (NN) to the task of NER. We attempt to learn a simplified NER task of the binary classification of a word as a person (1) or not (0).

We learn both a single hidden layer NN architecture and a double hidden layer NN architecture. After optimizing the hyperparameters and training schedule of the model, we are able to achieve an F1 score of 93.2\% on the token level evaluation metric and an F1 score of 88.0\% on the entity level evaluation metric.  These results are particularly impressive given that we encode no specific information about natural language rules or features to the model. 

\subsection{Supplemental Material}
In the Appendix \ref{backprop} of this paper we present a full derivation of the backpropagation algorithm for finding the gradients of the parameters of a single layer NN.  

We also prove in Appendix \ref{softmaxproof} that a softmax classifier with only 2 classes is equivalent to logistic regression with a single weight vector.

Lastly in  Appendix \ref{tsne} we include visualizations of the word vector representations both before and after training our model with the optimal architecture and parameters.  We use the t-SNE algorithm for dimensionality reduction \cite{tsne}.


\section{Model}\label{model}
We learn and tune both a single and double hidden layer Nueral Network.  The cost function for both NNs is given by
\begin{equation}\label{bigcost}
\begin{split}
J(\theta) = \frac{1}{m} \sum_{i=1}^m \left[-y ^{(i)} log(h_{\theta}(x^{(i)}))-(1-y^{(i)})log(1-h_{\theta}(x^{(i)}))\right]+\\ \frac{C}{2m} \left[ \sum_{j=1}^{nC} \sum_{k=1}^H W_{k,j}^2 + \sum_{k=1}^H U_{k}^2\right]
\end{split}
\end{equation}
The Feedforward function for the single hidden layer NN is
\begin{equation}\label{singleNN}
h_{\theta}(x^{(i)}) = g\left(U^Tf\left(Wx^{(i)}+b^{(1)}\right)+b^{(2)}\right)
\end{equation}
The Feedforward function for the double hidden layer NN is
\begin{equation}\label{doubleNN}
h_{\theta}(x^{(i)}) = g\left(U^T f\left( W^{(2)}f\left(W^{(1)}x^{(i)}+b^{(1)}\right) +b^{(2)}\right)+b^{(3)}\right)
\end{equation}
In both \eqref{singleNN} and \eqref{doubleNN}, \(f\) and \(g\) are given by the \(sigmoid\) and \(hyperbolic\) \(tangent\) functions respectively.

We use Stochastic Gradient Descent (SGD), \(\theta^{(t)} := \theta^{(t-1)} - \alpha\frac{\partial}{\partial\theta^{(t-1)}}J_i(\theta)\), to find the values of the parameters \(W^{(i)}, U, b^{(i)}\) and \(L\) which minimize \eqref{bigcost} and thereby maximize the log-likelihood of our data.  Here \(L\) is the word vector matrix containing \(n\)-dimensional pretrained word vectors for each word in the vocabulary (in this paper \(n=50\)).

In order to implement SGD we must first derive the gradient of \eqref{bigcost} with respect to the above parameters.  Appendix \ref{backprop} steps through the backpropagation algorithm in order to derive these parameters.

We also implement the numerical gradient calculation method in order to verify the correctness of our gradient derivations and implementations in both the single and double hidden layer NNs.  We find that the norm of the difference between the numerically computed and the analytically computed gradients is significantly less than \(10^{-7}\), on the order of \(10^{-9}\), thus we are confident in the correctness of our gradient derivation and implementation.

In \ref{netanalysis} we explore the results found tuning the model hyperparameters for both NN architectures. These include the hidden layer size, the window size, the learning rate and the regularization constant.

The input vector \(x^{(i)}\) to our model represents an \(n*C\) dimensional column vector where \(C\) is the contextual window size for inputs to the model, i.e. the \(C-1\) words that surround the word we atttempt to classify in the training corpus.  We explore various tunings of this window size in \ref{netanalysis} as well.  

We implement and observe the difference between using the full document as the context or each sentence as the context.  In practice this is done using start and end symbols which are placed in windows that overlap the beginning and end of the document or the beginning and end of each sentence.

Our final model appends a capitalization vector \cite{collobert} to each word vector corresponding to one of four capitalization scenarios.  These scenarios are: all lowercase, first word capitalized, all uppercase and at least one other than the first letter is uppercase.  We use a capitalization matrix \(LC\in\mathbb{R}^{5 \times 4}\) in order to store and update these parameters.

\section{Netwok Analysis}\label{netanalysis}
We now discuss the tuning of the Neural Network architecture and hyper-parameters in order to optimize F1 in the NER classification task.\footnote[1]{We use the following short form to represent the hyperparameters used in the neural network run. (C = [regularization constant] W = [window size] H = [hidden layer size] A = [learning rate]).}

Implementing the capitalization feature vectors as described in the section \ref{model} and \cite{collobert} drastically improved both the train and dev F1 scores as seen in \ref{capfig}.  Clearly this implies that NER for person classification relies heavily on word capitalization information.  Previously the model was unable to distinguish between a capitalized and non-capitalized word which actually contains a lot of information about the classification of the word since “person” words tend to be uppercased in the first letter.  Thus we add this feature vector to each word vector in tuning the rest of our model hyper parameters.

\begin{figure}[ht]
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{tex_figures/cap.png}
\caption{F1 scores of model with the presence and absence of capitalization matrix ( C = 0.0 W = 5 H = 100 A = 0.01)}
\label{capfig}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{tex_figures/sentence.png}
\caption{F1 scores of model with document and sentence based start and end tokens ( C = 0.0 W = 5 H = 100 A = 0.01)}
\label{sentencefig}
\end{minipage}
\end{figure}


Varying where we place our start and end tokens did not have much of an effect on our F1 score. We see from Figure \ref{sentencefig} that although the sentence model performs slightly better than the document based model, due to the randomness of initial values and the random permutation optimization implemented for SGD (described in Section \ref{furtheropt}) this can be attributed to the inherent noise in our system. In the mean, varying this parameter does not show significant changes to our system. Thus this feature is not added to our system before tuning other hyper parameters.





The next hyperparameter we tuned was the learning rate for the SGD update step.  Figures \ref{learntrain} and \ref{learndev} displays the learning curves for three different values of the learning rate.  The smallest of value, .0001, is not able to break out of bad local minima when optimizing and thus converges to a much lower F1 value than the two higher learning rates.  The learning rate of .001 as in \ref{learntrain} and \ref{learndev} is the most stable yet takes much longer to converge than .01 and thus we chose to use .01 as our final learning rate in tuning the other parameters.  Notice that with a learning rate of .01 our model is much more erratic as it oscillates about the minimum yet does not converge precisely.  We use this learning rate in order to get baseline results for tuning parameters, however, in tuning the final model we anneal our learning rate in order to precisely converge to the highest optimum.

\begin{figure}[ht]
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{tex_figures/atrain.png}
\caption{F1 scores of the training set varying the learning rate for stochastic gradient  ( C = 0.0001 W = 7 H = 100)}
\label{learntrain}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{tex_figures/atest.png}
\caption{F1 scores of the test set varying the learning rate for stochastic gradient  ( C = 0.0001 W = 7 H = 100)}
\label{learndev}
\end{minipage}
\end{figure}


After optimizing the input features and learning rate of the model we then tune the three hyper parameters: window size, hidden layer size and regularization.  We hold all other parameters to the model fixed and vary only the hyper parameter we attempt to optimize for.  In Figure \ref{winsize} we see that the model does not vary significantly with window size changes above size 5 with an optimal value at 7.  Thus we choose window size of 7 for our final model.
In Figure \ref{hiddensize} we see again that as we vary the hidden layer size the model performance does not change significantly.  However, we do note that the training time increases substantially as we increase hidden layer size.  Thus given the time to run and the fact that a hidden layer size of 100 performs as well or better than the others, we choose a hidden layer size of 100 for our optimal model.
In Figure \ref{varyregularization} we see that the model performance increases slightly with a small regularization weight (.0001) however drops off significantly if we increase the regularization constant above .001.  Thus we use regularization constant of 0.0001 for our optimal model.

\begin{figure}[ht]
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{tex_figures/windowsize.png}
\caption{F1 scores of train and dev sets with a 1 hidden layer model varying the window size. ( C = 0.0  H = 100 A = 0.01)}
\label{winsize}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{tex_figures/hiddensize.png}
\caption{F1 scores of train and dev sets with a 1 hidden layer model varying the hidden layer size. ( C = 0.0 W = 5  A = 0.01)}
\label{hiddensize}
\end{minipage}
\end{figure}

\begin{figure}[ht]
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{tex_figures/regularization.png}
\caption{F1 scores of train and dev sets with a 1 hidden layer model varying the regularization constant. ( W = 5  H = 100 A = 0.01)}
\label{varyregularization}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{tex_figures/HL.png}
\caption{F1 scores of train and dev sets with both the single and double hidden layer NN architectures. (C=0.001 W = 7  H = 100 A = 0.01)}
\label{numhl}
\end{minipage}
\end{figure}

After tuning some of our hyperparameters, we tested our NN model by adding a second hidden layer and running that model against our first hidden layer. The results can be seen in Figure \ref{numhl}. We find that the second hidden layer is more erratic on the training set (it has a higher standard deviation) but in general the results are comparable using both layers. It is worth noting that for our training set, even with one hidden we achieve near perfect score. Since our F1 score is already very high and our model learns the most from our L matrix, the incremental value that we have from adding a second hidden layer is not noticeable. However, after running our model over a number of different tests, we found that adding our second layer did slightly improve our score by a small amount.

The best result achieved was an F1 score of 93.2\%. Note that this is using a token level F1 evaluation metric. On an entity level, this score is 88.0\%. For this score we used the following hyperparameters: C = 0.0001, W = 7, H = 200, A = 0.01 (with annealing), K = 10.

\section{Further Optimizations}

This section outlines and describes several further optimzation techniques that we implemented in the model or chose not to implement and why.

The first optimization we added to the model was to randomly shuffle the training sample set before each epoch of SGD (an epoch is a full iteration through the training set).  The purpose of implementing this was to allow for smoother learning throughout the epoch rather than biasing the gradient with several related word updates at a single step.  We do not show the results here due to space constraints, but we found that the optimal score achieved using this method of learning was slightly higher and the learning throughout the epochs was much smoother and more predictably converged to an optimum.

Given the training F1 scores achieved in Figure \ref{numhl}, we see that the model fits the training set almost perfectly achieving an F1 of approximately 99\%.  We also note that the models do not noticeably overfit the test that despite perfect fitting of the training set.  From this we conclude that better optimization techiniques such as LBFGS or conjugate gradient methods would not allow the model to better fit the training data and likely would run slower since they calculate the gradient over the full training set over each iteration.  Thus we chose not to explore other optimization techiniques and conclude that SGD performance is not the bottle neck in achieving higher results.

In order to achieve the optimal results of F1=93.2\%token and F1=88\%entity as mentioned in \ref{netanalysis} we employ a technique known as annealing in order to ensure that our model converges perfectly to the optimal value for parameters.  We anneal the learning rate by an order of magnitude after the first 7 epochs and another order of magnitude after the first 9 epochs and converge to the optimal results.  SInce we start the model with a high learning rate of 0.01 we find that the SGD oscillates around an optimal value and never quite converges thus dropping the learning rate allows us to converge more precisely to the optimal value.


\section{Error Analysis}
-quick analysis of before/after tsne visualizations.

\newpage

\begin{thebibliography}{9}

\bibitem{tsne}
  L.J.P. van der Maaten and G.E. Hinton. Visualizing High-Dimensional Data Using t-SNE. 
  \emph{Journal of Machine Learning Research}
  9(Nov):2579-2605, 2008.

\bibitem{collobert}
  R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa.
  Natural Language Processing (Almost) from Scratch.
  \emph{Journal of Machine Learning Research}.
  12:2493–2537, 2011.


\end{thebibliography}

\newpage
\appendix
\appendixpage

\section{Backpropagation}\label{backprop}
We now derive the Backpropagation algorithm for finding the gradients of the cost function of the neural network.  For our parameters \(W\in\mathbb{R}^{H \times nC}\), \( b^{(1)} \in\mathbb{R}^H\), \(U\in\mathbb{R}^H\),\( b^{(2)} \in\mathbb{R}\) and \(L\in\mathbb{R}^{nC \times V}\) the cost function for our neural network is defined as follows.

\begin{equation}
\begin{split}
J(\theta) = \frac{1}{m} \sum_{i=1}^m \left[-y ^{(i)} log(h_{\theta}(x^{(i)}))-(1-y^{(i)})log(1-h_{\theta}(x^{(i)}))\right]+\\ \frac{C}{2m} \left[ \sum_{j=1}^{nC} \sum_{k=1}^H W_{k,j}^2 + \sum_{k=1}^H U_{k}^2\right]
\end{split}
\end{equation}
\begin{equation}
\begin{split}
h_{\theta}(x^{(i)}) = g\left(U^Tf\left(Wx^{(i)}+b^{(1)}\right)+b^{(2)}\right)
\end{split}
\end{equation}
\begin{equation}
f(z) = tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
\end{equation}
\begin{equation}
g(z) = sigmoid(z) = \frac{1}{1+e^{-z}}
\end{equation}
The derivatives of the \(tanh\) and \(sigmoid\) functions \(g\) and \(f\) are given by
\begin{equation}\label{tanhderiv}
\frac{d}{dz}f(z) = 1 - \left(\frac{e^z-e^{-z}}{e^z+e^{-z}}\right)^2 = 1- f^2(z)
\end{equation}
\begin{equation}\label{sigderiv}
\frac{d}{dz}g(z) = \frac{1}{1+e^{-z}}\left(1 - \frac{1}{1+e^{-z}}\right) = g(z)(1-g(z))
\end{equation}
Notice that the derivatives of both the hyperbolic tangent and logistic functions are defined completely in terms of the functions themselves.  This will simplify deriving the gradients for each parameter.  Since we will implement stochastic gradient descent in order to optimize our cost function with respect to the parameters \(U, b^{(2)}, W, b^{(1)}\) and \(L\) we only need to observe the gradient with respect to these parameters at a single training example at a time.  Thus we find the gradient for the simplified cost function below where \(x\) and \(y\) represent a single training example.
\begin{equation}\label{cost}
\begin{split}
J(\theta) = -ylog(h_{\theta}(x))-(1-y)log(1-h_{\theta}(x))+\\ \frac{C}{2} \left[ \sum_{j=1}^{nC} \sum_{k=1}^H W_{k,j}^2 + \sum_{k=1}^H U_{k}^2\right]
\end{split}
\end{equation}
We now derive the gradients \(\frac{\partial J}{\partial U}\),\(\frac{\partial J}{\partial b^{(2)}}\), \(\frac{\partial J}{\partial W}\), \(\frac{\partial J}{\partial b^{(1)}}\) and \(\frac{\partial J}{\partial L}\).  In order to simplify the gradient for each parameter notice that the regularization term (the second term in the cost function) does not depend on \(b^{(1)}\), \(b^{(2)}\) or \(L\) thus vanishes for those terms.  Also since the gradient distributes over addition we can take the derivative of this term with respect to \(U\) and \(W\) separately and add it in later.  The gradient of the regularization term for the two parameters is
\begin{equation}\label{ureg}
\frac{\partial}{\partial U}\frac{C}{2} \left[ \sum_{j=1}^{nC} \sum_{k=1}^H W_{k,j}^2 + \sum_{k=1}^H U_{k}^2\right] = CU
\end{equation}
\begin{equation}\label{wreg}
\frac{\partial}{\partial W}\frac{C}{2} \left[ \sum_{j=1}^{nC} \sum_{k=1}^H W_{k,j}^2 + \sum_{k=1}^H U_{k}^2\right] = CW
\end{equation}
To simplify notation we define \( z^{(1)}, z^{(2)}, a^{(1)}, a^{(2)} \) as follows.
\begin{equation}
\begin{aligned}
z^{(1)} =& Wx + b^{(1)}, \\
a^{(1)} =& f(z^{(1)}), \\
z^{(2)} =& U^T a^{(1)} + b^{(2)}, \\
a^{(2)} =& g(z^{(2)}) \\
\end{aligned}
\end{equation}
Finally we let the even further simplified cost function written in terms of the above defininitions without regularization (as we will add that in later) be defined as
\begin{equation}\label{simplecost}
J(U,b^{(2)},W,b^{(1)},L) = -ylog(a^{(2)})-(1-y)log(1-a^{(2)})
\end{equation}
First we derive \(\frac{\partial J}{\partial U}\) with respect to \eqref{simplecost}.  Using the chain rule and remembering the derivative of the sigmoid function from \eqref{sigderiv} we find
\begin{displaymath}
\begin{aligned}
\frac{\partial J}{\partial U} =&  \left(\frac{-y}{a^{(2)}}+\frac{(1-y)}{1-a^{(2)}}\right)\frac{\partial a^{(2)}}{\partial z^{(2)}} \frac{\partial z^{(2)}}{\partial U}, \\
\frac{\partial J}{\partial U} =& \left(\frac{-y}{a^{(2)}}+\frac{(1-y)}{1-a^{(2)}}\right)(a^{(2)})(1-a^{(2)}) \frac{\partial z^{(2)}}{\partial U}, \\
\frac{\partial J}{\partial U} =& \left(-y(1-a^{(2)})+(1-y)a^{(2)}\right)\frac{\partial z^{(2)}}{\partial U},
\end{aligned}
\end{displaymath}
\begin{equation}\label{partialU}
\frac{\partial J}{\partial U} = \left(a^{(2)} - y\right)\frac{\partial z^{(2)}}{\partial U}
\end{equation}
Now we only need to find the partial \( \frac{\partial z^{(2)}}{\partial U} \) in \eqref{partialU} in order to derive the gradient.
\begin{equation}\label{dzdu}
\frac{\partial z^{(2)}}{\partial U} = \frac{\partial }{\partial U}\left(U^Ta^{(1)}+b^{(2)}\right) = (a^{(1)})^T
\end{equation}
Putting together \eqref{partialU} and \eqref{dzdu} we arrive at
\begin{equation}\label{djdu}
\frac{\partial J}{\partial U} = \left(a^{(2)} - y\right)(a^{(1)})^T
\end{equation}
Also using the exact same steps to arrive at \eqref{partialU} we can solve for \( \frac{\partial J}{\partial b^{(2)}}\)
\begin{equation}\label{djdb2}
\frac{\partial J}{\partial b^{(2)}} = \left(a^{(2)} - y\right)\frac{\partial z^{(2)}}{\partial b^{(2)}}  = \left(a^{(2)} - y\right)
\end{equation}
We find \( \frac{\partial J}{\partial W}\) using the same derivation as in \eqref{partialU} and applying the chain rule again.
\begin{equation}\label{partialW}
\frac{\partial J}{\partial W} = \left(a^{(2)} - y\right)\frac{\partial z^{(2)}}{\partial W} = \left(a^{(2)} - y\right)\frac{\partial z^{(2)}}{\partial a^{(1)}}\frac{\partial a^{(1)}}{\partial z^{(1)}}\frac{\partial z^{(1)}}{\partial W}
\end{equation}
To simplify the derivation we take all of the partial derivatives in \eqref{partialW} with respect to a single element, \(W_{ij}\).  First notice that only the \(i^{th}\) element of \(a^{(1)}\) and \(z^{(1)}\) depend on the \(ij^{th}\) element of \(W\), thus we have
\begin{equation}\label{dJdW}
\frac{\partial J}{\partial W_{ij}} = \left(a^{(2)} - y\right)\frac{\partial z^{(2)}}{\partial a_{i}^{(1)}}\frac{\partial a_{i}^{(1)}}{\partial z_{i}^{(1)}}\frac{\partial z_{i}^{(1)}}{\partial W_{ij}}
\end{equation}
Now we can find each partial separately and combine them.
\begin{equation}\label{dz2da1}
\frac{\partial z^{(2)}}{\partial a_{i}^{(1)}} = \frac{\partial}{\partial a_{i}^{(1)}}\left(U^Ta^{(1)}+b^{(2)}\right) = U^T_{i}
\end{equation}
From the derivative of the hyperbolic tangent function found in \eqref{tanhderiv} we have
\begin{equation}\label{da1dz1}
\frac{\partial a_{i}^{(1)}}{\partial z_{i}^{(1)}} = \frac{\partial}{\partial z_{i}^{(1)}}f(z_i^{(1)})  = 1-f(z_i^{(1)})^2 = 1-(a_i^{(1)})^2
\end{equation}
\begin{equation}\label{dz1dw}
\frac{\partial z_{i}^{(1)}}{\partial W_{ij}} = \frac{\partial}{\partial W_{ij}} \left( Wx+b^{(1)} \right) = x_j
\end{equation}
Combining \eqref{dJdW}, \eqref{dz2da1}, \eqref{da1dz1} and \eqref{dz1dw} we end up with
\begin{equation}
\frac{\partial J}{\partial W_{ij}} = \left(a^{(2)} - y\right) U^T_{i} \left( 1-(a_i^{(1)})^2 \right) x_j 
\end{equation}
In order to find the gradient in matrix form we need to introduce new notation.  Let \(\diamond \) be defined as an element-wise multiplication of two matrices or vectors of the same size.  Now we have the full gradient as
\begin{equation}\label{djdw}
\frac{\partial J}{\partial W} = \left(a^{(2)} - y\right) U \diamond\left( 1-(a^{(1)})^2 \right) x^T
\end{equation}
To find \(\frac{\partial J}{\partial b^{(1)}}\) we apply the same steps we used to arrive at \eqref{dJdW}
\begin{equation}
\frac{\partial J}{\partial b^{(1)}_{i}} = \left(a^{(2)} - y\right)\frac{\partial z^{(2)}}{\partial a_{i}^{(1)}}\frac{\partial a_{i}^{(1)}}{\partial z_{i}^{(1)}}\frac{\partial z_{i}^{(1)}}{\partial b_{i}^{(1)}}
\end{equation}
\begin{equation}\label{dz1db1}
\frac{\partial z_{i}^{(1)}}{\partial b_{i}^{(1)}} = \frac{\partial}{\partial b^{(1)}_{i}} \left( Wx+b^{(1)} \right) = 1
\end{equation}
We combine \eqref{dJdW}, \eqref{dz2da1}, \eqref{da1dz1} and \eqref{dz1db1} and find
\begin{equation}\label{djdb1}
\frac{\partial J}{\partial b^{(1)}} = \left(a^{(2)} - y\right) U \diamond\left( 1-(a^{(1)})^2 \right)
\end{equation}
In order to find the last parameter gradient \( \frac{\partial J}{\partial L} \), we find \( \frac{\partial J}{\partial x} \) where \(x\) is the training example and then map this gradient to its corresponding piece of the \( \frac{\partial J}{\partial L} \) gradient.  We use the same steps as \eqref{dJdW} but this time every element of \(a^{(1)} \) and \( z^{(1)}\) depend on the \(i^{th}\) element of \(x\).
\begin{equation}
\frac{\partial J}{\partial x_{i}} = \left(a^{(2)} - y\right)\frac{\partial z^{(2)}}{\partial a^{(1)}}\frac{\partial a^{(1)}}{\partial z^{(1)}}\frac{\partial z^{(1)}}{\partial x_{i}}
\end{equation}
Letting \( W_{\bullet i} \) designate the \(i^{th}\) column of \(W\),
\begin{equation}
\frac{\partial z^{(1)}}{\partial x_i} = \frac{\partial}{\partial x_i} \left( Wx+b^{(1)}\right) = W_{\bullet i}
\end{equation}
Using the same results for the partial derivatives from \eqref{dz2da1}, \eqref{da1dz1} and the above two equations we have
\begin{equation}\label{djdxi}
\frac{\partial J}{\partial x_i} = \left(a^{(2)} - y\right) \left[U \diamond\left( 1-(a^{(1)})^2 \right)\right]^T W_{\bullet i}
\end{equation}
To find \(\frac{\partial J}{\partial x_i}\) in vector form notice that \eqref{djdxi} gives a row vector if we multiply by the full matrix \(W\) thus we simply need the transpose of this row vector to arrive at the column vector gradient for \(x\)
\begin{equation}\label{djdx}
\frac{\partial J}{\partial x} = \left(a^{(2)} - y\right) W^T \left[ U \diamond\left( 1-(a^{(1)})^2 \right)\right]
\end{equation}
To see how to map \eqref{djdx} back to \( \frac{\partial J}{\partial L}\) notice that \(x\) is an \( nC\) column vector where \(n\) is the vector size of each word and \(C\) is the window size or number of context words in each sample.  This means we have \(C\) columns of \( L \) to update for each training sample.  Thus we can write \(x\) as a \(C\) long vector with each element corresponding to a word vector.
\begin{displaymath}
x = \begin{bmatrix}  x_{1}\\  x_{2} \\ \vdots \\ x_{C}\end{bmatrix}
\end{displaymath}
Each of these \(C\)  \(n\)-dimensional vectors, \(x_{i} \), correspond to a column of \(L\).  Thus we can update the columns of \(L\) using this breakdown of word vectors of \(x\) and the gradient found in \eqref{djdx}.  In order to do this we must keep track what column each of the word vectors of \(x\) correspond to.

Our final derivations for \(\frac{\partial J}{\partial b^{(1)}}\), \(\frac{\partial J}{\partial b^{(2)}}\) and \(\frac{\partial J}{\partial L}\) are given in \eqref{djdb1}, \eqref{djdb2} and \eqref{djdx} respectively since these gradients don't depend on our regularization term.  To find the final form of \(\frac{\partial J}{\partial W}\) we combine \eqref{wreg} and \eqref{djdw}.  Similarly to find the final form of \(\frac{\partial J}{\partial U}\) we combine \eqref{ureg} and \eqref{djdu}.
\begin{equation}
\frac{\partial J}{\partial W} = \left(a^{(2)} - y\right) U \diamond\left( 1-(a^{(1)})^2 \right) x^T +CW
\end{equation}
\begin{equation}
\frac{\partial J}{\partial U} = \left(a^{(2)} - y\right)(a^{(1)})^T + CU
\end{equation}
This completes the derivation of the parameters to the cost function \eqref{cost} for our neural network model.

\section{Softmax and Logistic Regression}\label{softmaxproof}

We now show that the Softmax classification model with only two classes is equivalent to logistic regression model with a single weight vector.  We begin with the general Sofmax model and setting the number of classes to 2 we then derive logistic regression with a single weight vector.

Recall that Sofmax classification is defined as
\begin{equation}\label{softmax}
p(y=i|x;\theta) = \phi_{i} = \frac{e^{\theta^T_{i}x}}{\sum_{j=1}^k \theta^T_{j}x}
\end{equation}
Thus the hypothesis function \( h_{\theta}(x) \) should output a \(k\)-dimensional vector of probabilities that \(x\) belongs to each class as defined in \eqref{softmax} where \(k\) represents the number of classes.  However, since the probability over all classes must sum to one, we have that \(p(y=k|x;\theta) = 1 - \sum_{i=1}^{k-1} p(y=i|x;\theta) = 1 -\sum_{i=1}^{k-1}  \phi_{i}\).  Since \(p(y=k|x;\theta)\) can be expressed as a function of all other \(k-1\) probabilities, our hypothesis function only needs to output the first \(k-1\) probabilities.
\begin{equation}
h_{\theta}(x) = \begin{bmatrix}  \phi_{1}\\  \phi_{2} \\ \vdots \\ \phi_{k-1}\end{bmatrix}
\end{equation}
Now we set \(k=2\) since we are considering Softmax with only two classes and we obtain
\begin{equation}
\begin{aligned}\label{logderivation}
h_{\theta}(x) &= \phi_{1} = \frac{e^{\theta^T_{1}x}}{\sum_{j=1}^2 \theta^T_{j}x} \\
h_{\theta}(x) &= \frac{e^{\theta^T_{1}x}}{e^{\theta^T_{1}x}+e^{\theta^T_{2}x}} \\
h_{\theta}(x) &= \frac{e^{\theta^T_{1}x}}{e^{\theta^T_{1}x}+e^{\theta^T_{2}x}} * \frac{\frac{1}{e^{\theta^T_{1}x}}}{\frac{1}{e^{\theta^T_{1}x}}} \\
h_{\theta}(x) &= \frac{1}{1+\frac{e^{\theta^T_{2}x}}{e^{\theta^T_{1}x}}} \\
h_{\theta}(x) &= \frac{1}{1+e^{\left(\theta_{2}-\theta_{1}\right)^Tx}} \\
h_{\theta}(x) &= \frac{1}{1+e^{-\left(\theta_{1}-\theta_{2}\right)^Tx}}
\end{aligned}
\end{equation}
We see that the final result derived in \eqref{logderivation} is exactly equivalent to the hypothesis function for   logistic regression with a single weight matrix, namely \(\theta = \theta_{1}-\theta_{2} \).

\section{Word Vector Visualization Using t-SNE}\label{tsne}


\section{Collaboration}
We both worked on the code and the report together. We pair programmed the entire code base as a team, and wrote the report as a team as well.



\end{document}